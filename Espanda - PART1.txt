User: how much of the lidarr project do we have here to be able to build the plugin?

Assistant: We have the full Lidarr repo checked out as a git submodule at `lm-bridge-plugin/Submodules/Lidarr`. It’s a complete working tree (`.git`, `src/`, `frontend/`, build scripts, etc.) and it’s **not** shallow. It’s pinned at commit `f6a3e7370540cc25caf3aaf0f1c91e7c085585ac` (tagged `v2.0.0.4645-3759-gf6a3e73705`), which is exactly what `lm-bridge-plugin/build.sh` expects (`Submodules/Lidarr/src`).

There’s also a `lm-bridge-plugin/Submodules/Lidarr.bak` copy, but the build uses the `Lidarr` submodule path.

If you want, I can show the exact version info or verify the plugin build path in `lm-bridge-plugin/build.sh`.

User: no need. I just wanted to know what we had available for discussion and reference work/exploration.

Assistant: Got it. In short: we have the full Lidarr submodule available for reference and exploration, plus the `upstream/lidarr-metadata` submodule. If you want to dive into anything specific later, just point me at it.

User: give me a light overview of the process by which lidarr gets a list of release groups for an artist, what data is included in that and then anything about update/refresh processes that obtain new release groups that weren't already present.  and whether or not previous are also updated as part of  that update

Assistant: **Overview**
- Lidarr treats “release groups” as `Album` records (MusicBrainz release-group MBIDs live in `Album.ForeignAlbumId`). The list for an artist is fetched in `SkyHookProxy.GetArtistInfo` via the metadata server endpoint `artist/{mbid}` and mapped into `Artist.Albums`. See `lm-bridge-plugin/Submodules/Lidarr/src/NzbDrone.Core/MetadataSource/SkyHook/SkyHookProxy.cs`.
- The artist response includes `ArtistResource.Albums` (list of `AlbumResource`) and Lidarr filters that list using the artist’s metadata profile (primary types, secondary types, release statuses). See `FilterAlbums` in `SkyHookProxy.cs`.

**What’s In The Release-Group List**
- Each “release group” is an `AlbumResource` with fields like `Id`, `OldIds`, `Title`, `Type`, `SecondaryTypes`, `ReleaseStatuses`, `ReleaseDate`, `Genres`, `Overview`, `Disambiguation`, `Images`, `Links`, `Rating`, `Artists`, and optional `Releases`. See `lm-bridge-plugin/Submodules/Lidarr/src/NzbDrone.Core/MetadataSource/SkyHook/Resource/AlbumResource.cs`.
- When mapped to Lidarr’s `Album` model, those fields are carried over (including `Images`, `Links`, `Ratings`, `Genres`, etc.). If `Releases` are present, they’re mapped into `AlbumRelease` objects with tracks/media filtering and a monitored release choice. See `MapAlbum`/`MapRelease` in `SkyHookProxy.cs` and `ReleaseResource.cs`.

**How New Release Groups Are Discovered**
- On artist refresh (`RefreshArtistService`), Lidarr re-fetches the artist info, diffs the remote album list vs local, and adds any new release groups. See `GetRemoteData`, `GetRemoteChildren`, and `SortChildren` in `lm-bridge-plugin/Submodules/Lidarr/src/NzbDrone.Core/Music/Services/RefreshArtistService.cs` and `RefreshEntityServiceBase.cs`.
- It respects import-list exclusions and old MBIDs when deciding whether to add/merge. See `GetRemoteChildren` in `RefreshArtistService.cs`.

**Do Existing Release Groups Get Updated?**
- Yes, but with nuance:
  - The artist refresh always re-syncs the *album list* (release-group metadata). Existing albums are marked “Updated” if their metadata differs. This updates fields like title, types, statuses, etc.
  - Album *details* (releases/tracks) are refreshed through `RefreshAlbumService`, which calls the metadata server `album/{id}` endpoint. This can be full or selective depending on trigger. See `RefreshAlbumService.cs`.
- Manual refresh forces album refresh; scheduled/automatic refresh can skip some albums based on “recent updates” from the metadata server (`recent/artist`, `recent/album`) or time-based heuristics. See `GetChangedArtists`/`GetChangedAlbums` in `SkyHookProxy.cs` and `ShouldRefreshArtist.cs`/`ShouldRefreshAlbum.cs`.

**Do Missing Release Groups Get Removed?**
- If a release group disappears from the metadata source, Lidarr can delete it **only if** it was not manually added and has no files. See `ShouldDelete` in `RefreshAlbumService.cs`.  
- There’s also UI guidance that non-excluded albums may be re-added on refresh if the artist has a metadata profile set (see localization string in `NzbDrone.Core/Localization/Core/en.json`).

If you want, I can map this into a quick sequence diagram and include the exact endpoints Lidarr hits in order.

User: not necessary. 

Assistant: Understood. If you want to dive into a specific part later, just point me at it.

User: So I'm going to paste a few paragraphs outlining an idea I'm brainstorming. I want to know if this is best handled by your agent mode or the planner mode right now.  SO here we go:



General overview:
I have an idea and I'm looking into the feasibility of implementing it within the scope of what's possible right now with the lmbridge being between lidarr and musicbrainz database. And whether there's another path to the solution/idea if what excists right now is not sufficient to accomplish it. That's a generic overview.

Now more specifically:

I’m trying to figure out how to collect two (or more) editions of an album
Original release (the original sound = high priority)
Deluxe or Anniversary with more tracks
Remasters (sometimes include subtle rearrangements)

Sometimes these deluxe or other editions show up as a different album entirely in Lidarr. I’m guessing it’s because they have their own release group in MusicBrainz but I’m not certain. But typically they’re part of the same release group as the original, even though their release dates may be years or decades apart. And when Lidarr creates its album object under the artist, you don’t have a choice to have more than one version. You can change the selection in the release menu, but that’s a single item you’re choosing.

One specific example album is “Up to Our Hips” from the Charlatans. It has MBID c141cab8-a116-375a-ab39-5f081acf4c30

The original was released in 1994 and has 10 tracks on one media (CD). There are numerous media issues for this in the release group from MB. 

Then there is a special edition titled “Up to Our Hips (30th Anniversary Expanded Edition)” and it has 20 tracks across 2 media (2x Digital Media). The release pop-up menu in Lidarr doesn’t display dates for the specific releases/issues, but I know this one was released in 2024.

The date info for all the releases/issues is in the JSON from MusicBrainz db query.

So my idea first as a BIG PICTURE concept:

Allow our bridge to detect such releases based on one or more defined patterns (that we create). This pattern matching would support ON/OFF state, so completely optional.

Supposing our pattern is simply “Anniversary” then for the above example the bridge would detect/see that there is a plain (original) issue and an Anniversary issue.  Multiple of each, potentially different formats, etc.

Up to this point I think it’s all very plausible as it’s really nothing more than data parsing and some regex at a basic level. Scoring to fine-tune gets a little more complex, especially for debugging/testing.

But the real challenge, the “missing link” is now how to make Lidarr see this as N albums (let’s say 2 in the above example).

I have one idea, however crazy it may be. And that’s to split out the special release into its own release group by giving it its own MBID, whether it be a completely new one or one we fashion from the original so we can easily decipher it as a split release and deal with it in the future on refresh or other queries.

MusicBrainz would not have that info itself. We’re not changing MusicBrainz database to create incompatibilities. We’re not replicating this “fake” MBID out to anyone else. It’s strictly a private and local convenience to allow this specific instance of Lidarr to track what we want in our personal collection.

This may be best handled with an additional database that can leverage the existing postgres setup the same way the LM-cache DB does. That would allow us to contain the original MBID and our new “fake” one without having to have the fake one have any special decoding ability - it can be just another genuinely generated code.

This is as far as I’ve gotten in documenting this idea….

Assistant: Short answer: **Planner mode** is the better fit if you want a structured feasibility assessment plus a concrete roadmap (data model, hook points, refresh behavior, edge cases). If you just want quick discussion/brainstorming, Default mode is fine.

I can switch to Planner mode and produce a scoped plan if you want. If yes, tell me:
1. Do you want a purely conceptual feasibility plan, or a plan that includes specific code touchpoints in this repo?

Quick initial take (no deep dive yet): this is plausible via LM‑Bridge response MITM or DB hooks. You can synthesize extra “album” entries by splitting release groups into virtual groups and keeping a stable mapping table (real MBID → synthetic MBID). The hard part is refresh consistency, not parsing.

Your call on mode.

User: conceptual to start is best.

Assistant: Here’s a **conceptual feasibility plan** (no code yet), tuned to your idea and the LM‑Bridge architecture.

**Conceptual Feasibility**
- **Yes, it’s feasible** to make Lidarr “see” N albums from a single MusicBrainz release group. The bridge can synthesize additional release‑group entries by partitioning the releases in the JSON it already serves.
- The right place to do this is **LM‑Bridge response shaping**, not Lidarr. You can do it in the MITM hook (response transform) or in DB hooks (query transform). MITM is simpler to prototype and reversible.

**Recommended High‑Level Approach**
1. **Partition releases inside a release group.**
   - Input: release list from MB (title, disambiguation, date, media, track count, etc.).
   - Output: partitions like `original`, `anniversary`, `deluxe`, `remaster`.
   - Rules: regex + scoring + “best candidate” selection. Ensure **each release goes to exactly one partition**.

2. **Create synthetic release‑group entries.**
   - For each partition, synthesize a new “Album” record.
   - Give it a **stable synthetic MBID** (UUIDv5 over `base_release_group_id + partition_key`). This avoids a DB just for IDs.
   - Keep the real MBID as the “source” reference internally.

3. **Serve synthetic album details.**
   - When Lidarr requests `album/{syntheticId}`, LM‑Bridge should respond with the **same release group**, but only the releases in that partition.

4. **Refresh behavior**
   - On artist refresh, the album list is recomputed from current MB data. New partitions become new synthetic albums.
   - For existing synthetic albums, refresh uses the same source MBID + partition rules to update the release list.
   - If a partition disappears (e.g., no matching releases anymore), Lidarr will “lose” that album on refresh. Whether it’s deleted depends on Lidarr’s deletion rules (files present, manual add, etc.).

**Alternative: Small Mapping DB (Optional)**
- If you want manual overrides, persistent user tagging, or a “do not recompute this” flag, add a tiny mapping table:
  - `source_release_group_id`
  - `synthetic_id`
  - `partition_key`
  - `enabled`
  - `last_evaluated`
- This gives you deterministic IDs plus the ability to keep or suppress a synthetic album even if heuristic results change.

**Key Tradeoffs**
- **Deterministic IDs (no DB)** are simpler and mostly sufficient if rules are stable.
- **Mapping DB** gives you control when rules evolve, but adds persistence, migrations, and UI/CLI decisions.
- **MITM response hook** is the least invasive. If it works, you can later migrate to DB hooks for performance or clarity.

**Known Lidarr Behaviors To Factor In**
- Lidarr treats release groups as albums. It expects one “album” per release group.
- It can **monitor only one release per album**. Your solution gives you multiple albums so you can monitor multiple releases.
- Refresh triggers:
  - Manual refresh always re‑queries.
  - Automatic refresh uses recent updates + timing heuristics. If you need partitions to appear quickly, make sure artists are considered “changed” in the bridge.

If you want next steps, I can turn this into a tighter conceptual spec (data fields, ID format, partition scoring rules, refresh invariants).

User: a question one one specific thing you mentioned...

#2 I believe it was
when you said "Keep the real MBID as the “source” reference internally."

internally how, without persistent storage?

Assistant: Short answer: **you can’t reliably keep a source MBID across requests without either persistence or a reversible ID scheme.** Inside a single request it’s trivial; across requests it isn’t.

Here are the practical options:

**Option A: Stateless, Reversible ID**
- Use a synthetic ID format that **encodes the source MBID + partition key** so the bridge can decode it later.
- This only works if Lidarr will accept non‑MBID IDs (or MBID‑looking IDs that are actually encoded). That’s risky; Lidarr generally treats these as GUIDs.

**Option B: Deterministic ID + In‑Memory Cache**
- Generate a UUIDv5 from `source_mbid + partition_key`.
- Store a **temporary mapping** in memory (synthetic → source + partition).
- This fails across restarts or cache expiry. It’s workable for short sessions, brittle for real use.

**Option C: Minimal Persistence (Recommended)**
- Store a tiny mapping table (even in LM‑Bridge cache DB):  
  `synthetic_id`, `source_mbid`, `partition_key`.
- This is the only reliable way to resolve `album/{syntheticId}` without relying on guesswork.

**What “internally” meant**
- During response shaping you always have the original MBID and can embed it in the payload (e.g., `OldIds`, `Disambiguation`, `Links`), but Lidarr won’t send it back to you later. So it helps for **debugging**, not for **lookup**.

If you want, I can outline a no‑DB approach that still works most of the time, but it will be fragile on restarts.

User: I think the most stable, lidarr-change/acceptance proof design requires persistent storage in the form of a mapping table at the very least

Assistant: Agreed. A persistent mapping table is the most stable and Lidarr‑proof approach. It gives you:

- Stable `synthetic_id` ↔ `source_mbid` ↔ `partition_key` resolution across restarts.
- Control over rule changes (keep or retire existing synthetic albums).
- A clean way to implement `album/{syntheticId}` lookups.

If you want, I can draft a minimal schema + lifecycle (create/update/retire logic) and how refresh should behave with it.

User: about re-using the cache db...  I'm hesitant on that as cache implies transient information that may have ttl expiration, deterministic lifecycle, etc. it's also something that one may want to nuke/clear in an emergency. and it would really be a BAD IDEA to lose our mapping table if it's used for a significant number of albums

Assistant: That’s a good instinct. A mapping table shouldn’t live in a cache if it’s authoritative.

Better persistence options:

1. **Dedicated DB/table in the same Postgres cluster**  
   - Separate schema or database (e.g., `lmbridge.mapping`).  
   - No TTL, no cache purge semantics.  
   - Easy to back up alongside your MB/Lidarr infra.

2. **Small SQLite DB in a persistent LM‑Bridge volume**  
   - Simple, zero external dependency.  
   - Good if you want a self‑contained bridge with minimal ops.  
   - Still durable, but you should include it in backups.

3. **Dedicated Postgres DB just for LM‑Bridge state**  
   - Clean separation from cache and MB data.  
   - More setup, but safest long‑term.

I’d avoid reusing the cache DB unless you can **guarantee no TTL and no “clear cache” path touches the mapping table**.

If you want, tell me which persistence route you prefer and I’ll outline a minimal schema + lifecycle rules.

User: I think from a maintenance and debugging perspective #2 actually makes the most sense.  sqlite is more portable, easy to peek into, easy to edit and trivial to back up.

Assistant: Agreed. SQLite in a persistent LM‑Bridge volume is a solid fit for maintenance and debugging.

If you want a concrete starting point, I’d suggest:

**SQLite Location**
- Persist in the LM‑Bridge volume, e.g. `data/lmbridge-state.db`.
- Enable WAL mode for concurrency and resilience.

**Minimal Schema (Conceptual)**
- `release_group_split`
  - `synthetic_id` (TEXT, PK)
  - `source_release_group_id` (TEXT, indexed)
  - `partition_key` (TEXT)
  - `partition_label` (TEXT)
  - `enabled` (INTEGER)
  - `rule_version` (TEXT)
  - `created_at` (TEXT)
  - `updated_at` (TEXT)
  - `meta_json` (TEXT) for debug metadata (regex hit, score, etc.)

**Lifecycle**
- On refresh, compute partitions and upsert mappings.
- If a partition no longer exists, mark `enabled=0` but don’t delete.
- Keep IDs stable unless you intentionally “rebuild mappings”.

If that’s aligned, I can draft a tighter conceptual spec for refresh and lookup behavior.

User: we're in alignment. just one small pause/question. how much of the original RG/release data do we need to replicate into our DB if any at all?  If we're not, then I assume given what we pull from our DB we then back-reference to musicbrainz to get all the particulars.

And lastly how will this information fit into the current caching scheme?

Assistant: Short answer: **you don’t need to replicate release data in SQLite at all** if you’re comfortable re‑reading MB data and partitioning on the fly. The DB only needs the mapping and (optionally) some debug metadata.

**Minimal DB contents**
- `synthetic_id → source_release_group_id + partition_key (+ rule_version)`
- Optional debug: last matched release IDs, sample release title, score explanation.

**How it works without storing releases**
1. Lidarr asks for `album/{syntheticId}`.
2. Bridge looks up `syntheticId` in SQLite → gets `source_release_group_id + partition_key`.
3. Bridge fetches the real release group data from MB (via existing LM‑Bridge path).
4. Bridge filters releases according to `partition_key` and returns the synthetic album.

**When you *might* store release data**
- If you want to “freeze” a synthetic album even if MB changes.
- If you want consistent results across refresh rules changes.
- If you want to avoid re‑partitioning on every request (but that’s usually cheap).

**Caching fit**
- Keep **SQLite mapping** entirely separate from cache.
- Let existing MB response caching work as‑is.
- Partitioning can be done **after cache**: reuse cached MB response, then filter.
- Optional: add a tiny **in‑memory cache** for partition results keyed by  
  `(source_release_group_id, partition_key, rule_version, mb_cache_etag)`.

So the stable core is: **mapping in SQLite, data from MB**, partitioned at request time. This keeps storage minimal and still leverages current caching effectively.

If you want, I can outline the exact request flow and cache keys as a 1‑page spec.

User: You just brought up a very significant and prescient point:

"If you want consistent results across refresh rules changes."

THIS is of critical importance. Just like the example I gave of the undefined lifecycle of cache storage, it's would be alarming to lose a long-tracked album because one inadvertantly changed a splitting rule in such a way as it would opt-out that album

Assistant: You’re 100% right to call that out. If rule changes can silently “un‑split” albums, the system becomes untrustworthy. The design should **guarantee stability** unless the user explicitly chooses to rebuild.

Here’s a safe policy:

**Stability First (Recommended)**
- Every synthetic album gets a **persistent mapping** with:
  - `synthetic_id`
  - `source_release_group_id`
  - `partition_key`
  - `rule_version`
  - `release_ids_json` (the exact MB release IDs included at creation)
  - `mode` = `frozen` or `dynamic`

**Behavior**
- **`frozen` (default):**  
  Never re‑partition due to rule changes. Always serve the stored release list.  
  If a release disappears from MB, drop it silently but keep the album.
- **`dynamic`:**  
  Recompute partitions only if `rule_version` matches current rules.  
  If rules changed, **do not auto‑recompute**. Mark as “needs migration”.

**Explicit Migration**
- Provide an explicit “rebuild” action (per artist / per album / global) that:
  - recomputes with new rules
  - overwrites `release_ids_json`
  - bumps `rule_version`

This way: **no album ever disappears because someone changed a regex**. Changes only apply when you opt in.

If you like this policy, I can draft the exact minimal schema and a refresh lifecycle that enforces it.

User: It would be a good idea to have, at least during development, a setting to immediately unfreeze all or not freeze by default. that way one can test rules and the perfomance of the implementation before letting it loose in making many new albums.

I also think the rules need to allow matching on multuiple keys to be able to scope rules individually: such as making the rule for one or more specific artists, requiring the date of the synthetic release to be within a certain range of the original. Certain track count threshold. And of course regex on title which would typically be the main differentiator.

Assistant: That all makes sense, and it’s easy to support without over‑engineering.

**Dev Flexibility (Freeze Controls)**
- Add a global setting like `split_mode` with values `frozen` / `dynamic` / `dry_run`.
- Add a developer override to **unfreeze all** or **default to dynamic** for testing.
- Make “freeze at creation” the production default, but allow opt‑out per rule.

**Rule System Capabilities (What you asked for)**
- **Scoping keys**: artist MBID, artist name regex, label, primary/secondary types, release status.
- **Date rules**: release date within `N` years of the “original” release group date.
- **Track count rules**: min/max track count, or delta vs original.
- **Title/Disambiguation regex**: “Anniversary”, “Deluxe”, “Expanded”, “Remaster”, etc.
- **Format rules**: media format or number of media.

**Recommendation: Split Rule Types**
- **Partition rules** decide *which bucket* a release goes into (original, deluxe, remaster, etc.).
- **Eligibility rules** decide *whether this release group can be split at all* (e.g., only for specific artists, only if multiple candidate partitions exist).

**Base/Original Selection (Suggested Heuristic)**
- Default original = earliest `official` release date with valid tracklist.
- Tie‑breaker = fewer tracks, fewer media, or non‑special title (no regex hit).
- Provide per‑rule override to force a specific baseline for a given artist.

**Practical Rule Schema (Conceptual)**
- `rule_id`, `enabled`, `priority`, `artist_scope`, `title_regex`, `disambiguation_regex`
- `date_window_years`, `track_count_delta`, `media_count_range`
- `partition_key` (original/deluxe/anniversary/remaster)
- `freeze_policy` (inherit/global/force frozen/dynamic)

**Safety Defaults**
- If rules change: no automatic re‑partition for existing synthetic albums.
- Require explicit “rebuild mappings” action to apply new rules.

If you want, I can draft a simple rule schema and a decision flow chart so you can sanity‑check it before we implement anything.

User: OK, start drafting.  one key point is that the bucket names should be flexible enough to allow changing.  it might be that a rule will split at most only Deluxe. It's not a necessity to split 4 ways for example.

User: <turn_aborted>
The user interrupted the previous turn on purpose. Any running unified exec processes were terminated. If any tools/commands were aborted, they may have partially executed; verify current state before retrying.
</turn_aborted>

User: and sorry, I didn't mean to imply changing the names of persistant buckets later on, I meant such that a bucket is not so specifically named that it can't deal with or is inappropriate for some release match we may come up with later

Assistant: Below is a first **conceptual draft** that incorporates flexible bucket semantics, non‑mandatory multi‑split, and freeze‑safe behavior. I kept it schematic so we can refine.

**Draft Concept Spec: Release‑Group Splitting**

**Goals**
- Allow optional splitting of a single MusicBrainz release group into N synthetic albums.
- Preserve stability across rule changes (no surprise deletions).
- Keep bucket semantics flexible (not tied to “deluxe/remaster/etc.”).

**Core Concepts**
- **Release group** = MB release group (base entity).
- **Synthetic album** = “virtual” album derived from a subset of releases in the group.
- **Partition** = a bucket of releases defined by matching rules.

**Bucket Flexibility**
- Buckets are defined by **`partition_key`** (opaque, user‑defined string) plus an optional **`partition_label`** (display name).
- `partition_key` is stable and persisted. `partition_label` can change without breaking IDs.
- A rule can produce only **one** bucket (e.g., just “variant_1”), no requirement to split 4 ways.

**Data Model (SQLite)**
- `release_group_split`
  - `synthetic_id` (TEXT, PK)
  - `source_release_group_id` (TEXT, indexed)
  - `partition_key` (TEXT)  
  - `partition_label` (TEXT, optional)
  - `mode` (TEXT: `frozen` or `dynamic`)
  - `rule_version` (TEXT)
  - `release_ids_json` (TEXT)  
  - `created_at`, `updated_at`
  - `meta_json` (TEXT, debug)

**Rules (Conceptual Schema)**
- `rule_id`, `enabled`, `priority`
- `scope`  
  - `artist_ids`  
  - `artist_name_regex`  
  - `exclude_artist_ids`
- `eligibility`  
  - `min_partitions` (e.g., 2)  
  - `min_releases`  
  - `allowed_release_status`  
  - `allowed_primary_types`
- `match`  
  - `title_regex`  
  - `disambiguation_regex`  
  - `label_regex`  
  - `track_count_min`, `track_count_max`  
  - `track_count_delta` (vs baseline)  
  - `media_count_min`, `media_count_max`  
  - `date_window_years` (vs baseline)  
  - `format_regex`
- `output`  
  - `partition_key` (opaque string)  
  - `partition_label` (optional label template)  
  - `freeze_policy` (`inherit`, `force_frozen`, `force_dynamic`)

**Baseline Selection (Original)**
- Baseline release defaults to the **earliest official** release with a valid tracklist.
- Tie‑breakers: fewer tracks, fewer media, no regex hits.
- Baseline used only for relative rules (date window, track delta).

**Partitioning Algorithm (High‑Level)**
1. Load releases for release group.
2. Compute baseline release.
3. For each rule in priority order:
   - If scope/eligibility match, assign matching releases to `partition_key`.
4. Unassigned releases go to `partition_key = "primary"` (or no synthetic split at all).
5. Create synthetic album for each non‑empty partition.

**Stability Rules**
- **Default `mode = frozen`** for newly created synthetic albums.
- `release_ids_json` captures the exact releases at creation.
- Rule changes never auto‑reshape existing frozen partitions.
- Explicit “rebuild” is required to apply new rules to existing splits.

**Development Controls**
- Global setting: `split_mode`  
  - `frozen` (default)  
  - `dynamic` (auto‑recompute)  
  - `dry_run` (log only)
- Debug option: “Unfreeze all” (for test cycles).

**Notes On Bucket Naming**
- `partition_key` is intentionally not semantic. Example values:  
  - `variant_1`  
  - `special_cut`  
  - `bonus_tracklist`
- You can still label them “Anniversary” or “Deluxe” via `partition_label`, without coupling the system to those names.

If this aligns, next I can draft a simple rule config format and an example decision flow so you can sanity‑check it before we implement anything.

User: yes, aligned 100%

Assistant: Great. Next I can draft either:

1. A minimal rule config format with a concrete example (e.g., your Charlatans album).
2. A decision flow chart (stepwise logic) showing how a release group is split and frozen.

Tell me which you want first, or if you want both in that order.

User: both in that order

Assistant: **1) Minimal Rule Config Format + Example**

Here’s a compact, human‑editable YAML‑style concept. It maps cleanly to the schema we discussed.

```yaml
version: "ruleset-2026-02-19"
defaults:
  split_mode: frozen           # frozen | dynamic | dry_run
  baseline:
    prefer_status: ["official"]
    prefer_earliest: true
    tie_breakers: ["fewer_tracks", "fewer_media", "no_title_hits"]
  fallback_partition_key: primary

rules:
  - id: "anniversary-expanded"
    enabled: true
    priority: 10
    scope:
      artist_ids: ["c141cab8-a116-375a-ab39-5f081acf4c30"]  # example scope
      artist_name_regex: null
      exclude_artist_ids: []
    eligibility:
      min_partitions: 1
      min_releases: 2
      allowed_release_status: ["official"]
      allowed_primary_types: ["album"]
    match:
      title_regex: "(anniversary|expanded|deluxe|special edition)"
      disambiguation_regex: "(anniversary|expanded|deluxe|special)"
      label_regex: null
      track_count_min: 12
      track_count_max: null
      track_count_delta: 4            # vs baseline
      media_count_min: 2
      media_count_max: null
      date_window_years: 40           # within 40y of baseline
      format_regex: null
    output:
      partition_key: "variant_1"
      partition_label: "Special / Expanded"
      freeze_policy: inherit

  - id: "remaster"
    enabled: true
    priority: 20
    scope:
      artist_ids: []
      artist_name_regex: null
      exclude_artist_ids: []
    eligibility:
      min_partitions: 1
      min_releases: 2
      allowed_release_status: ["official"]
      allowed_primary_types: ["album"]
    match:
      title_regex: "(remaster|remastered)"
      disambiguation_regex: "(remaster|remastered)"
      label_regex: null
      track_count_min: null
      track_count_max: null
      track_count_delta: null
      media_count_min: null
      media_count_max: null
      date_window_years: 50
      format_regex: null
    output:
      partition_key: "variant_2"
      partition_label: "Remaster"
      freeze_policy: inherit
```

Notes:
- `partition_key` is opaque. You can change `partition_label` any time without breaking IDs.
- Only one bucket is required. If only the “anniversary” rule matches, you get two albums: `primary` + `variant_1`.
- `min_partitions` can be set to 2 if you want to only split when two distinct buckets are populated.

---

**2) Decision Flow (Stepwise Logic)**

**Input**: release group ID  
**Output**: 1..N synthetic albums + mapping records

1. **Fetch release group data** (from MB via LM‑Bridge).
2. **Select baseline release**  
   - Prefer earliest official; tie‑breakers: fewer tracks, fewer media, no title hits.
3. **Initialize partitions**  
   - `partitions = {}`  
   - `unassigned = all releases`
4. **Evaluate rules in priority order**  
   - If scope + eligibility fail, skip.
   - For each release in `unassigned`:
     - If it matches rule’s criteria, assign it to `partition_key`.
5. **Assign remaining releases**
   - If any releases remain, assign to `fallback_partition_key` (`primary`).
6. **Eligibility check**
   - If total populated partitions < `min_partitions`, **do not split**; return single album.
7. **Create / update synthetic albums**
   - For each partition:
     - If mapping exists and `mode=frozen`, keep stored `release_ids`.
     - If mapping doesn’t exist: create mapping, freeze by default, store `release_ids`.
     - If `mode=dynamic` and rule_version matches: recompute + update.
8. **Serve results**
   - Artist listing includes synthetic albums (with synthetic IDs).
   - `album/{syntheticId}` returns only that partition’s releases.

---

If you want, I can next draft:
- A “migration” control plan (how to safely re‑partition when rules change).
- The minimal SQLite schema and a quick example row set.

User: ok for both of those one at a time

Assistant: **Migration Control Plan (Safe Re‑Partitioning)**

**Goal**: Apply new rules without breaking existing collections or losing albums unexpectedly.

**Modes**
- `frozen`: never auto‑repartition.
- `dynamic`: auto‑repartition only if `rule_version` matches current ruleset.
- `dry_run`: log results only.

**Migration Strategy**
1. **Ruleset versioning**
   - Every ruleset has a `version` string (e.g., `ruleset-2026-02-19`).
   - Each mapping stores `rule_version` at creation.

2. **Default behavior on rules change**
   - If current `ruleset.version` ≠ mapping `rule_version`:
     - Do **not** modify existing frozen mappings.
     - Mark them as `needs_migration = true` (stored in `meta_json` or a column).

3. **Explicit migration actions**
   - `migrate-all`: recompute and update all mappings.
   - `migrate-artist`: recompute for all release groups under one artist.
   - `migrate-album`: recompute a single release group/synthetic album.
   - `migrate-preview` (dry run): show the differences first.

4. **Safe update semantics**
   - Recompute partitions using new rules.
   - Update `release_ids_json` only if:
     - `mode = dynamic` OR explicit migration command.
   - Update `rule_version` after successful write.

5. **Retiring partitions**
   - If a partition disappears under new rules:
     - Do **not delete** mapping automatically.
     - Mark `enabled = 0` and keep in DB.
     - Require explicit “prune retired partitions” to remove.

6. **Backups**
   - Before migration, save `lmbridge-state.db` copy.
   - Optional: export a `mappings_snapshot.json` for diffs.

This gives you a controlled, reversible lifecycle for rule changes.

---

If that works, next I’ll provide the **minimal SQLite schema + example rows**.

User: ok

Assistant: **Minimal SQLite Schema + Example Rows**

**Schema (minimal, stable)**
```sql
CREATE TABLE IF NOT EXISTS release_group_split (
  synthetic_id TEXT PRIMARY KEY,
  source_release_group_id TEXT NOT NULL,
  partition_key TEXT NOT NULL,
  partition_label TEXT,
  mode TEXT NOT NULL DEFAULT 'frozen',       -- frozen | dynamic
  rule_version TEXT NOT NULL,
  release_ids_json TEXT NOT NULL,            -- JSON array of MB release IDs
  enabled INTEGER NOT NULL DEFAULT 1,         -- 1=active, 0=retired
  created_at TEXT NOT NULL,
  updated_at TEXT NOT NULL,
  meta_json TEXT                             -- optional debug info
);

CREATE INDEX IF NOT EXISTS idx_rg_split_source
  ON release_group_split(source_release_group_id);

CREATE INDEX IF NOT EXISTS idx_rg_split_partition
  ON release_group_split(partition_key);
```

**Example Rows**
```sql
-- Base release group (primary partition)
INSERT INTO release_group_split (
  synthetic_id, source_release_group_id, partition_key, partition_label,
  mode, rule_version, release_ids_json, enabled, created_at, updated_at, meta_json
) VALUES (
  '0c9c3b3d-9a2f-5c23-bef1-8e2a2e9d0caa',
  'c141cab8-a116-375a-ab39-5f081acf4c30',
  'primary',
  'Original / Standard',
  'frozen',
  'ruleset-2026-02-19',
  '["mb-release-id-1","mb-release-id-2","mb-release-id-3"]',
  1,
  '2026-02-19T12:00:00Z',
  '2026-02-19T12:00:00Z',
  '{"baseline_release_id":"mb-release-id-1"}'
);

-- Special / Expanded partition
INSERT INTO release_group_split (
  synthetic_id, source_release_group_id, partition_key, partition_label,
  mode, rule_version, release_ids_json, enabled, created_at, updated_at, meta_json
) VALUES (
  '8b41f713-8e0b-5b0b-8c1f-0f43ef83de4f',
  'c141cab8-a116-375a-ab39-5f081acf4c30',
  'variant_1',
  'Special / Expanded',
  'frozen',
  'ruleset-2026-02-19',
  '["mb-release-id-9","mb-release-id-10"]',
  1,
  '2026-02-19T12:00:00Z',
  '2026-02-19T12:00:00Z',
  '{"rule_id":"anniversary-expanded","match_hits":["title_regex","track_count_delta"]}'
);
```

**Notes**
- `synthetic_id` can be UUIDv5 derived from `source_release_group_id + partition_key`. That makes it deterministic and stable.
- `partition_label` is cosmetic. You can change it without breaking IDs.
- `release_ids_json` is the key to stability across rule changes.
- `meta_json` is optional but very useful for debugging and migration previews.

If you want, next I can propose:
1. A deterministic synthetic ID scheme (UUIDv5 inputs).
2. A minimal CLI/API surface for “rebuild/migrate/prune”.

User: before we move onto those, can you write out the entire chat history into a file?
